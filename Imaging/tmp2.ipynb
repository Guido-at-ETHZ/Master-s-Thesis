{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdQlp62JzCJm5EsBL1s3ir"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8h_JbJ_Lu8W","outputId":"cde837f9-e3ca-4b0e-d3cb-93bdf14984e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Found 2 files to process\n","\n","Processing file 1/2: denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001.tif\n","Processing file: denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001.tif\n","Image shape: (3, 13, 1024, 1024), dtype: float32\n","Processing Cadherins without tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.0228258203715086, 7.299477577209473] to [0, 1]\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Processing Cadherins with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.0228258203715086, 7.299477577209473] to [0, 1]\n","Applying tophat background removal...\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Cadherins/background/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Cadherins_regional.tif\n","Saved confidence map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Cadherins/background/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Cadherins_regional_confidence.tif\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Cadherins/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Cadherins_regional_tophat.tif\n","Saved confidence map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Cadherins/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Cadherins_regional_tophat_confidence.tif\n","Processing Nuclei with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.017089147120714188, 5.5640387535095215] to [0, 1]\n","Applying tophat background removal...\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Nuclei/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Nuclei_regional_tophat.tif\n","Saved confidence map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Nuclei/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Nuclei_regional_tophat_confidence.tif\n","Processing Golgi without tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.007639906369149685, 5.611553192138672] to [0, 1]\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Processing Golgi with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.007639906369149685, 5.611553192138672] to [0, 1]\n","Applying tophat background removal...\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Golgi/background/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Golgi_regional.tif\n","Saved confidence map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Golgi/background/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Golgi_regional_confidence.tif\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Golgi/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Golgi_regional_tophat.tif\n","Saved confidence map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2/Golgi/tophat/denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001_Golgi_regional_tophat_confidence.tif\n","Successfully processed denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq001.tif\n","\n","Processing file 2/2: denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq002.tif\n","Processing file: denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq002.tif\n","Image shape: (3, 13, 1024, 1024), dtype: float32\n","Processing Cadherins without tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.03531668707728386, 4.41688871383667] to [0, 1]\n","Applying enhanced regional guided EDF...\n","Using adaptive region size: 25, overlap: 8\n","Error processing file denoised_1.4Pa_A1_19dec21_20xA_L2RA_FlatA_seq002.tif: <function _var_dispatcher at 0x7835907bd440> returned a result with an exception set\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import numpy as np\n","from skimage import io, filters, exposure, morphology, measure, segmentation, img_as_float, img_as_uint\n","from skimage.feature import peak_local_max\n","from scipy import ndimage\n","import glob\n","import tifffile\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","# ======= CONFIGURATION =======\n","# Set paths specifically for your Google Colab environment\n","INPUT_DIR = '/content/drive/MyDrive/knowledge/University/Master/Thesis/denoised_ordered/trial'\n","OUTPUT_DIR = '/content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial2'\n","\n","# Default EDF parameters\n","DEFAULT_MAX_Z_DIFF = 1      # Maximum allowed z-difference between adjacent pixels\n","DEFAULT_SIGMA = 1.0         # Sigma for Laplacian operator smoothing\n","DEFAULT_GAUSS_DENOISE = 0.5 # Sigma for Gaussian denoising\n","\n","# Define channel names (customize these based on your data)\n","CHANNEL_NAMES = ['Cadherins', 'Nuclei', 'Golgi']\n","\n","# ======= UTILITY FUNCTIONS =======\n","\n","def create_dirs():\n","    \"\"\"Create necessary directories for the new output structure\"\"\"\n","    os.makedirs(INPUT_DIR, exist_ok=True)\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","    # Create the new directory structure\n","    for channel in ['Cadherins', 'Nuclei', 'Golgi']:\n","        if channel == 'Nuclei':\n","            # Nuclei only has tophat\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'tophat'), exist_ok=True)\n","        else:\n","            # Cadherins and Golgi have both tophat and background\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'tophat'), exist_ok=True)\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'background'), exist_ok=True)\n","\n","def normalize_image(img):\n","    \"\"\"Normalize image to 0-1 range\"\"\"\n","    img_min = np.min(img)\n","    img_max = np.max(img)\n","    if img_max > img_min:\n","        return (img - img_min) / (img_max - img_min)\n","    return img\n","\n","# ======= BACKGROUND REMOVAL =======\n","\n","def tophat_filter_background(image, radius=15):\n","    \"\"\"Remove background using white top-hat filter\"\"\"\n","    # Normalize input to 0-1 range\n","    img = normalize_image(img_as_float(image))\n","\n","    # Create structuring element (disk)\n","    selem = morphology.disk(radius)\n","\n","    # Apply white top-hat filter (removes background while preserving foreground)\n","    tophat = morphology.white_tophat(img, selem)\n","\n","    # Enhance contrast and ensure 0-1 range\n","    tophat = exposure.rescale_intensity(tophat)\n","\n","    # Double-check normalization\n","    tophat = normalize_image(tophat)\n","\n","    return tophat\n","\n","# ======= ENHANCED FOCUS MEASURE CALCULATION =======\n","\n","def calculate_focus_measures(stack, gauss_denoise=0.5, sigma=1.0):\n","    \"\"\"Enhanced focus measure calculation using multiple methods\"\"\"\n","    z_size, height, width = stack.shape\n","    focus_measures = np.zeros((z_size, height, width))\n","\n","    for z in range(z_size):\n","        # Apply Gaussian filter to reduce noise\n","        if gauss_denoise > 0:\n","            blurred = filters.gaussian(stack[z], sigma=gauss_denoise)\n","        else:\n","            blurred = stack[z]\n","\n","        # 1. Laplacian (edge detection)\n","        laplacian = np.abs(filters.laplace(blurred, ksize=3))\n","\n","        # 2. Sobel gradient magnitude (another edge detector)\n","        sobel_h = filters.sobel_h(blurred)\n","        sobel_v = filters.sobel_v(blurred)\n","        sobel = np.sqrt(sobel_h**2 + sobel_v**2)\n","\n","        # 3. Variance (local contrast)\n","        variance = ndimage.generic_filter(blurred, np.var, size=3)\n","        variance = normalize_image(variance)\n","\n","        # Combine measures (weighted sum)\n","        combined = (0.5 * laplacian +\n","                   0.3 * normalize_image(sobel) +\n","                   0.2 * variance)\n","\n","        # Apply small Gaussian to make the decision more robust\n","        if sigma > 0:\n","            focus_measures[z] = filters.gaussian(combined, sigma=sigma)\n","        else:\n","            focus_measures[z] = combined\n","\n","    return focus_measures\n","\n","# ======= ADAPTIVE REGION SIZE =======\n","\n","def determine_adaptive_region_size(image, min_size=10, max_size=25):\n","    \"\"\"Determine appropriate region size based on image complexity\"\"\"\n","    # Calculate edge density\n","    edges = filters.sobel(image)\n","    edge_density = np.mean(edges > 0.05)  # Threshold for edge detection\n","\n","    # Scale region size inversely with edge density\n","    # More edges -> smaller regions for better detail\n","    if edge_density > 0.2:\n","        return min_size\n","    elif edge_density < 0.05:\n","        return max_size\n","    else:\n","        # Linear interpolation between min and max size\n","        return int(max_size - (edge_density - 0.05) * (max_size - min_size) / 0.15)\n","\n","# ======= BETTER SEED POINT SELECTION =======\n","\n","def select_seed_points(focus_measures, initial_best_z):\n","    \"\"\"Select better seed points based on focus confidence and local maxima\"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Calculate confidence (focus measure at the selected z-slice)\n","    confidence = np.zeros((height, width))\n","    for y in range(height):\n","        for x in range(width):\n","            z = initial_best_z[y, x]\n","            confidence[y, x] = focus_measures[z, y, x]\n","\n","    # Find local maxima in the confidence map\n","    local_max_coords = peak_local_max(\n","        confidence,\n","        min_distance=5,  # Minimum distance between peaks\n","        threshold_abs=0.1,  # Absolute threshold for peak detection\n","        exclude_border=2   # Exclude border pixels\n","    )\n","\n","    # Sort by confidence value (highest first)\n","    local_max_values = [confidence[y, x] for y, x in local_max_coords]\n","    sorted_indices = np.argsort(local_max_values)[::-1]\n","    sorted_seeds = [local_max_coords[i] for i in sorted_indices]\n","\n","    # Limit number of seeds to prevent over-segmentation\n","    max_seeds = max(10, int(0.01 * height * width))  # Max 1% of pixels as seeds, but at least 10\n","    return sorted_seeds[:max_seeds]\n","\n","# ======= ADAPTIVE Z-DIFFERENCE PARAMETER =======\n","\n","def calculate_adaptive_z_diff(focus_measures, initial_best_z, base_max_z_diff=1):\n","    \"\"\"Calculate adaptive max_z_diff based on local depth variations\"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Calculate local z-variance in small neighborhoods\n","    z_var = ndimage.generic_filter(\n","        initial_best_z.astype(float),\n","        np.var,\n","        size=5,  # 5x5 neighborhood\n","        mode='nearest'\n","    )\n","\n","    # Scale max_z_diff based on local variance\n","    # Higher variance -> larger z_diff allowed\n","    adaptive_z_diff = np.clip(\n","        base_max_z_diff + np.sqrt(z_var),\n","        base_max_z_diff,\n","        base_max_z_diff * 3  # Cap at 3x the base value\n","    ).astype(np.int32)\n","\n","    return adaptive_z_diff\n","\n","# ======= ENHANCED REGIONAL GUIDED EDF =======\n","\n","def edf_with_spatial_continuity_region_enhanced(focus_measures, max_z_diff=1):\n","    \"\"\"\n","    Enhanced version of spatial continuity with better seed selection\n","    \"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Find initial z-slice with maximum focus for each pixel\n","    initial_best_z = np.argmax(focus_measures, axis=0)\n","\n","    # Get better seed points\n","    seed_points = select_seed_points(focus_measures, initial_best_z)\n","\n","    # Create a processed mask to track pixels that have been assigned a final z-value\n","    processed = np.zeros((height, width), dtype=bool)\n","\n","    # Create the output z-map that will be filled with spatially consistent z-values\n","    final_best_z = np.copy(initial_best_z)\n","\n","    # Function to get valid 4-connected neighbors\n","    def get_neighbors(y, x):\n","        neighbors = []\n","        for ny, nx in [(y-1, x), (y+1, x), (y, x-1), (y, x+1)]:\n","            if 0 <= ny < height and 0 <= nx < width:\n","                neighbors.append((ny, nx))\n","        return neighbors\n","\n","    # Initialize frontier with seed points\n","    frontier = seed_points.copy()\n","    for y, x in frontier:\n","        processed[y, x] = True\n","\n","    # Process each point and its neighbors using a breadth-first approach\n","    while frontier:\n","        y, x = frontier.pop(0)\n","        current_z = final_best_z[y, x]\n","\n","        # Check neighbors\n","        for ny, nx in get_neighbors(y, x):\n","            if processed[ny, nx]:\n","                continue\n","\n","            # Find best z within allowed range from current pixel\n","            z_min = max(0, current_z - max_z_diff)\n","            z_max = min(z_size - 1, current_z + max_z_diff)\n","\n","            # Extract the relevant slice of focus measures and find best z\n","            z_slice = focus_measures[z_min:z_max+1, ny, nx]\n","            relative_best_z = np.argmax(z_slice)\n","            final_best_z[ny, nx] = z_min + relative_best_z\n","\n","            processed[ny, nx] = True\n","            frontier.append((ny, nx))\n","\n","    # Check if there are any unprocessed pixels left (should be rare)\n","    unprocessed = ~processed\n","    if np.any(unprocessed):\n","        # Assign them the value of nearest processed neighbor\n","        dist, indices = ndimage.distance_transform_edt(\n","            unprocessed, return_indices=True)\n","\n","        # Assign z-values from nearest processed pixel\n","        for y, x in zip(*np.where(unprocessed)):\n","            idx_y, idx_x = indices[0, y, x], indices[1, y, x]\n","            final_best_z[y, x] = final_best_z[idx_y, idx_x]\n","\n","    return final_best_z\n","\n","def edf_regional_guided_enhanced(image_stack, base_max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                                sigma=DEFAULT_SIGMA, gauss_denoise=DEFAULT_GAUSS_DENOISE):\n","    \"\"\"\n","    Enhanced regional guided EDF with adaptive parameters\n","    \"\"\"\n","    # Convert to float and ensure 0-1 range\n","    stack = img_as_float(image_stack)\n","    stack = normalize_image(stack)\n","    z_size, height, width = stack.shape\n","\n","    # Determine adaptive region size based on middle slice\n","    mid_z = z_size // 2\n","    adaptive_region_size = determine_adaptive_region_size(stack[mid_z])\n","    overlap = max(3, adaptive_region_size // 3)  # Adaptive overlap\n","\n","    print(f\"Using adaptive region size: {adaptive_region_size}, overlap: {overlap}\")\n","\n","    # Calculate enhanced focus measures\n","    focus_measures = calculate_focus_measures(stack, gauss_denoise, sigma)\n","\n","    # Find initial best z-slices\n","    initial_best_z = np.argmax(focus_measures, axis=0)\n","\n","    # Calculate adaptive max_z_diff\n","    adaptive_z_diff = calculate_adaptive_z_diff(focus_measures, initial_best_z, base_max_z_diff)\n","\n","    # Create output arrays\n","    best_z = np.zeros((height, width), dtype=np.float64)\n","    weights = np.zeros((height, width), dtype=np.float32)\n","\n","    # Process image in overlapping regions\n","    for y_start in range(0, height, adaptive_region_size - overlap):\n","        for x_start in range(0, width, adaptive_region_size - overlap):\n","            # Define region bounds\n","            y_end = min(y_start + adaptive_region_size, height)\n","            x_end = min(x_start + adaptive_region_size, width)\n","\n","            # Extract region\n","            region_focus = focus_measures[:, y_start:y_end, x_start:x_end]\n","            region_z_diff = adaptive_z_diff[y_start:y_end, x_start:x_end]\n","\n","            # Get maximum z_diff for this region\n","            max_z_diff_region = int(np.ceil(np.mean(region_z_diff)))\n","\n","            # Process region with spatial continuity\n","            region_best_z = edf_with_spatial_continuity_region_enhanced(\n","                region_focus,\n","                max_z_diff=max_z_diff_region\n","            )\n","\n","            # Create weight mask (higher weights in center, lower at edges)\n","            y_grid, x_grid = np.mgrid[y_start:y_end, x_start:x_end]\n","            y_center = (y_start + y_end) / 2\n","            x_center = (x_start + x_end) / 2\n","\n","            # Calculate distance from center (normalized to 0-1)\n","            y_dist = np.abs(y_grid - y_center) / (adaptive_region_size / 2)\n","            x_dist = np.abs(x_grid - x_center) / (adaptive_region_size / 2)\n","            dist = np.maximum(y_dist, x_dist)\n","            region_weights = np.clip(1.0 - dist, 0.1, 1.0)\n","\n","            # Convert region_best_z to float64 before multiplying\n","            region_best_z_float = region_best_z.astype(np.float64)\n","\n","            # Add weighted contribution to output\n","            best_z[y_start:y_end, x_start:x_end] += region_best_z_float * region_weights\n","            weights[y_start:y_end, x_start:x_end] += region_weights\n","\n","    # Normalize by weights and round to nearest integer\n","    best_z = np.round(best_z / np.maximum(weights, 1e-6)).astype(np.int32)\n","\n","    # Ensure best_z values are within valid range\n","    best_z = np.clip(best_z, 0, z_size - 1)\n","\n","    # Create output by taking pixels from best z-slices\n","    result = np.zeros((height, width), dtype=np.float32)\n","    for z in range(z_size):\n","        mask = best_z == z\n","        result[mask] = stack[z][mask]\n","\n","    # Create a visualization of the focus map (normalized to 0-1)\n","    focus_map = best_z / (z_size - 1)\n","\n","    return result, focus_map, best_z\n","\n","# ======= FOCUS MAP REFINEMENT =======\n","\n","def refine_focus_map(best_z, confidence, smoothness=0.5):\n","    \"\"\"\n","    Refine the focus map to reduce noise while preserving edges\n","    \"\"\"\n","    # Create edge-preserving filter\n","    edges = filters.sobel(confidence)\n","    edge_mask = edges > np.percentile(edges, 80)  # Top 20% of edge responses\n","\n","    # Apply bilateral filter to smooth similar regions while preserving edges\n","    refined = filters.gaussian(best_z, sigma=smoothness)\n","\n","    # Keep original values at edge locations\n","    refined[edge_mask] = best_z[edge_mask]\n","\n","    return refined\n","\n","# ======= MAIN PROCESSING FUNCTIONS =======\n","\n","def process_single_stack(stack, apply_tophat=False, max_z_diff=DEFAULT_MAX_Z_DIFF):\n","    \"\"\"\n","    Process a single z-stack with the enhanced regional EDF method\n","    \"\"\"\n","    print(f\"Processing stack with shape {stack.shape}...\")\n","\n","    # Convert to float and ensure valid range (0-1)\n","    stack = img_as_float(stack)\n","\n","    # Ensure stack values are in 0-1 range\n","    min_val = np.min(stack)\n","    max_val = np.max(stack)\n","    if min_val < 0 or max_val > 1:\n","        print(f\"Normalizing stack from range [{min_val}, {max_val}] to [0, 1]\")\n","        stack = normalize_image(stack)\n","\n","    # Apply background removal if requested\n","    if apply_tophat:\n","        print(\"Applying tophat background removal...\")\n","        processed_stack = np.zeros_like(stack, dtype=np.float32)\n","        for z in range(stack.shape[0]):\n","            processed_stack[z] = tophat_filter_background(stack[z])\n","        working_stack = processed_stack\n","    else:\n","        working_stack = stack\n","\n","    # Store original slices for comparison\n","    mid_z_idx = stack.shape[0] // 2\n","\n","    # Prepare results dictionary\n","    results = {\n","        'original_middle': stack[mid_z_idx],\n","        'original_max': np.max(stack, axis=0)\n","    }\n","\n","    # Apply enhanced regional method\n","    print(\"Applying enhanced regional guided EDF...\")\n","    proj, focus_map, best_z = edf_regional_guided_enhanced(\n","        working_stack, base_max_z_diff=max_z_diff)\n","\n","    # Calculate focus confidence for refinement\n","    focus_measures = calculate_focus_measures(working_stack)\n","    confidence = np.zeros_like(best_z, dtype=np.float32)\n","    for y in range(best_z.shape[0]):\n","        for x in range(best_z.shape[1]):\n","            z = best_z[y, x]\n","            confidence[y, x] = focus_measures[z, y, x]\n","\n","    # Apply focus map refinement for smoother results\n","    refined_best_z = refine_focus_map(best_z, confidence, smoothness=0.75)\n","\n","    # Create refined projection using the refined focus map\n","    refined_proj = np.zeros_like(proj)\n","    for z in range(working_stack.shape[0]):\n","        mask = np.round(refined_best_z).astype(np.int32) == z\n","        refined_proj[mask] = working_stack[z][mask]\n","\n","    results['regional_edf'] = refined_proj\n","    results['focus_map_regional'] = focus_map\n","\n","    # Add focus confidence map for debugging/visualization\n","    results['focus_confidence'] = confidence\n","\n","    return results\n","\n","def save_results_to_disk(results, filename, channel_name, apply_tophat):\n","    \"\"\"Save results to disk with updated directory structure and naming\"\"\"\n","    base_name = os.path.splitext(filename)[0]\n","\n","    # Get only the projection result (not focus map)\n","    key = \"regional_edf\"\n","    if key in results:\n","        # Determine processing type folder\n","        process_type = \"tophat\" if apply_tophat else \"background\"\n","\n","        # Skip if this is not one of the required outputs\n","        if channel_name == \"Nuclei\" and not apply_tophat:\n","            print(f\"Skipping {channel_name} without tophat (not required)\")\n","            return\n","\n","        # Create appropriate directory path\n","        output_dir = os.path.join(OUTPUT_DIR, channel_name, process_type)\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Create output filename - using just the base name as requested\n","        output_filename = f\"{base_name}_{channel_name}_regional\"\n","        if apply_tophat:\n","            output_filename += \"_tophat\"\n","        output_filename += \".tif\"\n","\n","        output_path = os.path.join(output_dir, output_filename)\n","\n","        # Normalize to 0-1 range and convert to uint16\n","        projection = results[key]\n","        projection_normalized = normalize_image(projection)\n","        projection_uint = img_as_uint(projection_normalized)\n","\n","        # Save\n","        tifffile.imwrite(output_path, projection_uint)\n","        print(f\"Saved {output_path}\")\n","\n","        # Additionally save focus confidence map for debugging (optional)\n","        if 'focus_confidence' in results:\n","            conf_path = output_path.replace('.tif', '_confidence.tif')\n","            conf_normalized = normalize_image(results['focus_confidence'])\n","            conf_uint = img_as_uint(conf_normalized)\n","            tifffile.imwrite(conf_path, conf_uint)\n","            print(f\"Saved confidence map: {conf_path}\")\n","\n","def process_directory(input_dir=INPUT_DIR, max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                     file_pattern='*.tif*', save_results=True):\n","    \"\"\"\n","    Process all files in a directory with channel-specific settings\n","    \"\"\"\n","    # Get all matching files\n","    all_files = glob.glob(os.path.join(input_dir, file_pattern), recursive=True)\n","    all_files.sort()\n","\n","    if not all_files:\n","        print(f\"No files matching '{file_pattern}' found in {input_dir}\")\n","        return []\n","\n","    print(f\"Found {len(all_files)} files to process\")\n","\n","    # Create output directories\n","    create_dirs()\n","\n","    # Process each file\n","    processed_files = []\n","\n","    for file_idx, file_path in enumerate(all_files):\n","        print(f\"\\nProcessing file {file_idx+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n","\n","        try:\n","            results = process_file(\n","                file_path, max_z_diff=max_z_diff,\n","                save_results=save_results\n","            )\n","\n","            if results:\n","                processed_files.append(file_path)\n","                print(f\"Successfully processed {os.path.basename(file_path)}\")\n","\n","        except Exception as e:\n","            print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    print(f\"\\nSuccessfully processed {len(processed_files)}/{len(all_files)} files\")\n","    return processed_files\n","\n","def process_file(file_path, max_z_diff=DEFAULT_MAX_Z_DIFF, save_results=True):\n","    \"\"\"\n","    Process a single file with channel-specific processing:\n","    - Nuclei (channel 1): Apply tophat only\n","    - Cadherins and Golgi (channels 0 and 2): Create with and without tophat\n","    \"\"\"\n","    filename = os.path.basename(file_path)\n","    print(f\"Processing file: {filename}\")\n","\n","    # Load image\n","    try:\n","        image = tifffile.imread(file_path)\n","        print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n","    except Exception as e:\n","        print(f\"Error loading image: {e}\")\n","        return None\n","\n","    # Handle different dimensionalities\n","    if len(image.shape) == 3:\n","        # Single channel z-stack - process with and without tophat\n","        results_no_tophat = process_single_stack(\n","            image, apply_tophat=False, max_z_diff=max_z_diff)\n","\n","        results_tophat = process_single_stack(\n","            image, apply_tophat=True, max_z_diff=max_z_diff)\n","\n","        if save_results:\n","            save_results_to_disk(results_no_tophat, filename, \"default\", False)\n","            save_results_to_disk(results_tophat, filename, \"default\", True)\n","\n","        return {\"default_no_tophat\": results_no_tophat, \"default_tophat\": results_tophat}\n","\n","    elif len(image.shape) == 4:\n","        # Multi-channel z-stack with channel-specific processing\n","        all_results = {}\n","\n","        # Process each channel with appropriate settings\n","        for ch_idx in range(image.shape[0]):\n","            # Extract channel\n","            channel_data = image[ch_idx]\n","            ch_name = CHANNEL_NAMES[ch_idx] if ch_idx < len(CHANNEL_NAMES) else f\"channel_{ch_idx}\"\n","\n","            # Apply channel-specific processing\n","            if ch_idx == 1:  # Nuclei (second channel) - tophat only\n","                print(f\"Processing {ch_name} with tophat...\")\n","                results = process_single_stack(\n","                    channel_data, apply_tophat=True, max_z_diff=max_z_diff)\n","\n","                all_results[f\"{ch_name}_tophat\"] = results\n","\n","                if save_results:\n","                    save_results_to_disk(results, filename, ch_name, True)\n","\n","            else:  # Cadherins and Golgi (channels 0 and 2) - with and without tophat\n","                print(f\"Processing {ch_name} without tophat...\")\n","                results_no_tophat = process_single_stack(\n","                    channel_data, apply_tophat=False, max_z_diff=max_z_diff)\n","\n","                print(f\"Processing {ch_name} with tophat...\")\n","                results_tophat = process_single_stack(\n","                    channel_data, apply_tophat=True, max_z_diff=max_z_diff)\n","\n","                all_results[f\"{ch_name}_no_tophat\"] = results_no_tophat\n","                all_results[f\"{ch_name}_tophat\"] = results_tophat\n","\n","                if save_results:\n","                    save_results_to_disk(results_no_tophat, filename, ch_name, False)\n","                    save_results_to_disk(results_tophat, filename, ch_name, True)\n","\n","        return all_results\n","\n","    else:\n","        print(f\"Unsupported image dimensions: {len(image.shape)}\")\n","        return None\n","\n","# ======= VISUALIZATION FUNCTIONS =======\n","\n","def visualize_focus_map(focus_map, original_image, projected_image, title=\"Focus Map Visualization\"):\n","    \"\"\"\n","    Create a visualization of the focus map as a heatmap overlay\n","    \"\"\"\n","    plt.figure(figsize=(18, 6))\n","\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(original_image, cmap='gray')\n","    plt.title(\"Original Image (Middle Slice)\")\n","    plt.axis('off')\n","\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(projected_image, cmap='gray')\n","    plt.title(\"Enhanced EDF Projection\")\n","    plt.axis('off')\n","\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(projected_image, cmap='gray', alpha=0.7)\n","    plt.imshow(focus_map, cmap='inferno', alpha=0.5)\n","    plt.colorbar(label='Z-Stack Position (Normalized)')\n","    plt.title(\"Focus Map Overlay\")\n","    plt.axis('off')\n","\n","    plt.suptitle(title)\n","    plt.tight_layout()\n","    return plt.gcf()\n","\n","# Main execution - just call process_directory with your parameters\n","def main():\n","    create_dirs()\n","    processed_files = process_directory(\n","        input_dir=INPUT_DIR,\n","        max_z_diff=DEFAULT_MAX_Z_DIFF,\n","        file_pattern='*.tif*',\n","        save_results=True\n","    )\n","    print(f\"Processed {len(processed_files)} files\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}