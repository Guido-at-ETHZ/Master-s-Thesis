{"cells":[{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import numpy as np\n","from skimage import io, filters, exposure, morphology, measure, segmentation, img_as_float, img_as_uint, transform\n","from scipy import ndimage\n","import glob\n","import tifffile\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from skimage.feature import peak_local_max\n","\n","# ======= CONFIGURATION =======\n","# Set paths specifically for your Google Colab environment\n","INPUT_DIR = '/content/drive/MyDrive/knowledge/University/Master/Thesis/denoised_ordered/trial'\n","OUTPUT_DIR = '/content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4'\n","\n","# Default EDF parameters\n","DEFAULT_MAX_Z_DIFF = 1      # Maximum allowed z-difference between adjacent pixels\n","DEFAULT_SIGMA = 1.0         # Sigma for Laplacian operator smoothing\n","DEFAULT_GAUSS_DENOISE = 0.5 # Sigma for Gaussian denoising\n","DEFAULT_SCALES = [0.5, 1.0, 2.0]  # Scales for multi-scale focus measure\n","DEFAULT_MAX_REFINEMENT_DIST = 2  # Maximum distance to search during refinement\n","\n","# Define channel names (customize these based on your data)\n","CHANNEL_NAMES = ['Cadherins', 'Nuclei', 'Golgi']\n","\n","# ======= UTILITY FUNCTIONS =======\n","\n","def create_dirs():\n","    \"\"\"Create necessary directories for the new output structure\"\"\"\n","    os.makedirs(INPUT_DIR, exist_ok=True)\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","    # Create the new directory structure\n","    for channel in ['Cadherins', 'Nuclei', 'Golgi']:\n","        if channel == 'Nuclei':\n","            # Nuclei only has tophat\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'tophat'), exist_ok=True)\n","        else:\n","            # Cadherins and Golgi have both tophat and background\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'tophat'), exist_ok=True)\n","            os.makedirs(os.path.join(OUTPUT_DIR, channel, 'background'), exist_ok=True)\n","\n","def normalize_image(img):\n","    \"\"\"Normalize image to 0-1 range\"\"\"\n","    img_min = np.min(img)\n","    img_max = np.max(img)\n","    if img_max > img_min:\n","        return (img - img_min) / (img_max - img_min)\n","    return img\n","\n","def get_valid_neighbors(y, x, height, width):\n","    \"\"\"Get valid 4-connected neighbors\"\"\"\n","    neighbors = []\n","    for ny, nx in [(y-1, x), (y+1, x), (y, x-1), (y, x+1)]:\n","        if 0 <= ny < height and 0 <= nx < width:\n","            neighbors.append((ny, nx))\n","    return neighbors\n","\n","# ======= BACKGROUND REMOVAL =======\n","\n","def tophat_filter_background(image, radius=15):\n","    \"\"\"Remove background using white top-hat filter\"\"\"\n","    # Normalize input to 0-1 range\n","    img = normalize_image(img_as_float(image))\n","\n","    # Create structuring element (disk)\n","    selem = morphology.disk(radius)\n","\n","    # Apply white top-hat filter (removes background while preserving foreground)\n","    tophat = morphology.white_tophat(img, selem)\n","\n","    # Enhance contrast and ensure 0-1 range\n","    tophat = exposure.rescale_intensity(tophat)\n","\n","    # Double-check normalization\n","    tophat = normalize_image(tophat)\n","\n","    return tophat\n","\n","# ======= IMPROVED FOCUS MEASURE CALCULATION =======\n","\n","def enhanced_focus_measure(image_stack, gauss_denoise=DEFAULT_GAUSS_DENOISE):\n","    \"\"\"\n","    Calculate enhanced focus measures using multiple focus operators\n","    \"\"\"\n","    z_size, height, width = image_stack.shape\n","    focus_measures = np.zeros((z_size, height, width))\n","\n","    for z in range(z_size):\n","        # Apply Gaussian filter to reduce noise\n","        if gauss_denoise > 0:\n","            blurred = filters.gaussian(image_stack[z], sigma=gauss_denoise)\n","        else:\n","            blurred = image_stack[z]\n","\n","        # Calculate multiple focus measures\n","        # 1. Laplacian (edge detection)\n","        laplacian = np.abs(filters.laplace(blurred, ksize=3))\n","\n","        # 2. Sobel gradient magnitude (detects intensity changes)\n","        sobel_h = filters.sobel_h(blurred)\n","        sobel_v = filters.sobel_v(blurred)\n","        sobel_mag = np.sqrt(sobel_h**2 + sobel_v**2)\n","\n","        # 3. Local variance (texture measure)\n","        local_mean = filters.gaussian(blurred, sigma=1.5)\n","        local_var = filters.gaussian((blurred - local_mean)**2, sigma=1.5)\n","\n","        # Combine measures with weights\n","        combined = (0.5 * filters.gaussian(laplacian, sigma=1.0) +\n","                    0.3 * filters.gaussian(sobel_mag, sigma=1.0) +\n","                    0.2 * filters.gaussian(local_var, sigma=1.0))\n","\n","        focus_measures[z] = combined\n","\n","    return focus_measures\n","\n","def multi_scale_focus_measure(image_stack, scales=DEFAULT_SCALES):\n","    \"\"\"\n","    Calculate focus measures at multiple scales to better capture different feature sizes\n","    \"\"\"\n","    z_size, height, width = image_stack.shape\n","    combined_focus = np.zeros((z_size, height, width))\n","\n","    for scale in scales:\n","        # Calculate focus at this scale\n","        if scale != 1.0:\n","            # Resize images for different scales\n","            scaled_stack = np.zeros((z_size, int(height*scale), int(width*scale)))\n","            for z in range(z_size):\n","                scaled_stack[z] = transform.resize(\n","                    image_stack[z],\n","                    (int(height*scale), int(width*scale)),\n","                    anti_aliasing=True\n","                )\n","            focus_at_scale = enhanced_focus_measure(scaled_stack)\n","\n","            # Resize focus measures back to original size\n","            resized_focus = np.zeros((z_size, height, width))\n","            for z in range(z_size):\n","                resized_focus[z] = transform.resize(\n","                    focus_at_scale[z],\n","                    (height, width),\n","                    anti_aliasing=True\n","                )\n","            focus_at_scale = resized_focus\n","        else:\n","            focus_at_scale = enhanced_focus_measure(image_stack)\n","\n","        # Add to combined focus\n","        combined_focus += focus_at_scale\n","\n","    return combined_focus / len(scales)\n","\n","# ======= IMPROVED SEED SELECTION =======\n","\n","def improved_seed_selection(focus_measures, initial_best_z):\n","    \"\"\"\n","    Improved seed point selection with consistency checking\n","    \"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Calculate focus confidence\n","    confidence = np.zeros((height, width))\n","    local_consistency = np.zeros((height, width))\n","\n","    for y in range(height):\n","        for x in range(width):\n","            z = initial_best_z[y, x]\n","            # Primary confidence: how much better is best slice compared to others\n","            focus_values = focus_measures[:, y, x]\n","            sorted_focus = np.sort(focus_values)\n","            if len(sorted_focus) >= 2:\n","                confidence[y, x] = (sorted_focus[-1] - sorted_focus[-2]) / (sorted_focus[-1] + 1e-6)\n","\n","            # Local consistency: check if neighbors agree on best z\n","            z_counts = {}\n","            for ny, nx in get_valid_neighbors(y, x, height, width):\n","                nz = initial_best_z[ny, nx]\n","                z_counts[nz] = z_counts.get(nz, 0) + 1\n","\n","            # Higher consistency if more neighbors agree\n","            max_count = max(z_counts.values()) if z_counts else 0\n","            local_consistency[y, x] = max_count / 4.0  # Normalize by max possible neighbors\n","\n","    # Combined score\n","    combined_score = 0.7 * confidence + 0.3 * local_consistency\n","\n","    # Select top percentile as seeds\n","    seed_threshold = np.percentile(combined_score, 95)\n","    seed_indices = np.where(combined_score > seed_threshold)\n","    seed_points = list(zip(seed_indices[0], seed_indices[1]))\n","\n","    # Sort seed points by score (highest first)\n","    seed_points.sort(key=lambda idx: combined_score[idx[0], idx[1]], reverse=True)\n","\n","    return seed_points, combined_score\n","\n","# ======= ADAPTIVE REGION SIZE =======\n","\n","def adaptive_region_size(image_stack):\n","    \"\"\"\n","    Determine adaptive region sizes based on image features\n","    \"\"\"\n","    # Use middle slice for feature calculation\n","    mid_z = image_stack.shape[0] // 2\n","    mid_slice = image_stack[mid_z]\n","\n","    # Calculate edge density\n","    edges = filters.sobel(mid_slice)\n","\n","    # Calculate local feature density using a sliding window\n","    feature_density = filters.gaussian(edges, sigma=10.0)\n","\n","    # Normalize to range 5-30\n","    min_region = 5\n","    max_region = 30\n","    normalized = min_region + (max_region - min_region) * (1.0 - normalize_image(feature_density))\n","\n","    # Return as integer\n","    return normalized.astype(np.int32)\n","\n","# ======= IMPROVED SPATIAL CONTINUITY =======\n","\n","def edf_with_spatial_continuity_region(focus_measures, max_z_diff=DEFAULT_MAX_Z_DIFF):\n","    \"\"\"\n","    Enhanced spatial continuity with better region growing\n","    \"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Find initial z-slice with maximum focus for each pixel\n","    initial_best_z = np.argmax(focus_measures, axis=0)\n","\n","    # Get improved seed points\n","    seed_points, confidence_scores = improved_seed_selection(focus_measures, initial_best_z)\n","\n","    # Create a processed mask to track pixels that have been assigned a final z-value\n","    processed = np.zeros((height, width), dtype=bool)\n","\n","    # Create the output z-map that will be filled with spatially consistent z-values\n","    final_best_z = np.copy(initial_best_z)\n","\n","    # Initialize frontier with seed points\n","    frontier = seed_points.copy()\n","    for y, x in frontier:\n","        processed[y, x] = True\n","\n","    # Process each point and its neighbors using a breadth-first approach\n","    while frontier:\n","        y, x = frontier.pop(0)\n","        current_z = final_best_z[y, x]\n","\n","        # Check neighbors\n","        for ny, nx in get_valid_neighbors(y, x, height, width):\n","            if processed[ny, nx]:\n","                continue\n","\n","            # Find best z within allowed range from current pixel\n","            z_min = max(0, current_z - max_z_diff)\n","            z_max = min(z_size - 1, current_z + max_z_diff)\n","\n","            # Extract the relevant slice of focus measures and find best z\n","            z_slice = focus_measures[z_min:z_max+1, ny, nx]\n","            relative_best_z = np.argmax(z_slice)\n","            final_best_z[ny, nx] = z_min + relative_best_z\n","\n","            processed[ny, nx] = True\n","            frontier.append((ny, nx))\n","\n","    # Check if there are any unprocessed pixels left (should be rare)\n","    unprocessed = ~processed\n","    if np.any(unprocessed):\n","        # Assign them the value of nearest processed neighbor\n","        dist, indices = ndimage.distance_transform_edt(\n","            unprocessed, return_indices=True)\n","\n","        # Assign z-values from nearest processed pixel\n","        for y, x in zip(*np.where(unprocessed)):\n","            idx_y, idx_x = indices[0, y, x], indices[1, y, x]\n","            final_best_z[y, x] = final_best_z[idx_y, idx_x]\n","\n","    return final_best_z\n","\n","# ======= HYBRID SELECTION STRATEGY =======\n","\n","def hybrid_best_z_selection(focus_measures, max_z_diff=DEFAULT_MAX_Z_DIFF):\n","    \"\"\"\n","    Hybrid approach for z-selection using both confidence and spatial continuity\n","    \"\"\"\n","    z_size, height, width = focus_measures.shape\n","\n","    # Get initial z-indices\n","    initial_best_z = np.argmax(focus_measures, axis=0)\n","\n","    # Get seed points and confidence map\n","    seed_points, confidence_scores = improved_seed_selection(focus_measures, initial_best_z)\n","\n","    # Get consistency-based result\n","    consistency_z_map = edf_with_spatial_continuity_region(focus_measures, max_z_diff)\n","\n","    # Blend results based on confidence\n","    final_z_map = np.zeros_like(initial_best_z)\n","\n","    # Normalize confidence for blending\n","    blend_factor = np.clip(confidence_scores * 2, 0, 1)\n","\n","    for y in range(height):\n","        for x in range(width):\n","            # Use weighted selection based on confidence\n","            if blend_factor[y, x] > 0.7:\n","                final_z_map[y, x] = initial_best_z[y, x]  # Trust high confidence pixels\n","            else:\n","                final_z_map[y, x] = consistency_z_map[y, x]  # Otherwise use consistency\n","\n","    return final_z_map\n","\n","# ======= POST-PROCESSING REFINEMENT =======\n","\n","def refine_z_map(best_z_map, focus_measures, max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST):\n","    \"\"\"\n","    Refine the z-map by checking neighborhood focus values\n","    \"\"\"\n","    z_size, height, width = focus_measures.shape\n","    refined_map = np.copy(best_z_map)\n","\n","    for y in range(height):\n","        for x in range(width):\n","            current_z = int(best_z_map[y, x])\n","\n","            # Define search range\n","            z_min = max(0, current_z - max_refinement_dist)\n","            z_max = min(z_size - 1, current_z + max_refinement_dist)\n","\n","            # Get focus values in local z-neighborhood\n","            local_z_values = focus_measures[z_min:z_max+1, y, x]\n","\n","            # Find local maximum\n","            local_best_z = z_min + np.argmax(local_z_values)\n","\n","            # Update only if focus is significantly better\n","            if focus_measures[local_best_z, y, x] > focus_measures[current_z, y, x] * 1.1:\n","                refined_map[y, x] = local_best_z\n","\n","    return refined_map\n","\n","# ======= IMPROVED REGIONAL GUIDED EDF METHOD =======\n","\n","def improved_edf_regional_guided(image_stack, max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                                max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST):\n","    \"\"\"\n","    Improved regional guided EDF with enhanced slice selection\n","    \"\"\"\n","    # Convert to float and ensure 0-1 range\n","    stack = img_as_float(image_stack)\n","    stack = normalize_image(stack)\n","    z_size, height, width = stack.shape\n","\n","    # Calculate multi-scale focus measures\n","    print(\"Calculating multi-scale focus measures...\")\n","    focus_measures = multi_scale_focus_measure(stack)\n","\n","    # Get initial best z-indices\n","    initial_best_z = np.argmax(focus_measures, axis=0)\n","\n","    # Calculate adaptive region sizes\n","    print(\"Determining adaptive region sizes...\")\n","    region_sizes = adaptive_region_size(stack)\n","\n","    # Create output arrays\n","    best_z = np.zeros((height, width), dtype=np.float64)\n","    weights = np.zeros((height, width), dtype=np.float32)\n","\n","    # Process image in overlapping regions with adaptive sizes\n","    print(\"Processing in adaptive regions...\")\n","    min_overlap = 3  # Minimum overlap between regions\n","\n","    for y_start in range(0, height, 10):  # Use fixed step but adaptive region size\n","        for x_start in range(0, width, 10):\n","            # Get adaptive region size for this area\n","            y_center, x_center = y_start + 5, x_start + 5\n","            if 0 <= y_center < height and 0 <= x_center < width:\n","                region_size = int(region_sizes[y_center, x_center])\n","            else:\n","                region_size = 15  # Default\n","\n","            # Ensure minimum size\n","            region_size = max(region_size, 5)\n","\n","            # Calculate overlap\n","            overlap = max(min_overlap, region_size // 3)\n","\n","            # Define region bounds\n","            y_end = min(y_start + region_size, height)\n","            x_end = min(x_start + region_size, width)\n","\n","            # Skip too small regions\n","            if y_end - y_start < 3 or x_end - x_start < 3:\n","                continue\n","\n","            # Extract region focus measures\n","            region_focus = focus_measures[:, y_start:y_end, x_start:x_end]\n","\n","            # Apply hybrid selection for this region\n","            region_best_z = hybrid_best_z_selection(region_focus, max_z_diff)\n","\n","            # Create weight mask (higher weights in center, lower at edges)\n","            y_grid, x_grid = np.mgrid[y_start:y_end, x_start:x_end]\n","            y_center = (y_start + y_end) / 2\n","            x_center = (x_start + x_end) / 2\n","\n","            # Calculate distance from center (normalized to 0-1)\n","            y_dist = np.abs(y_grid - y_center) / (region_size / 2)\n","            x_dist = np.abs(x_grid - x_center) / (region_size / 2)\n","            dist = np.maximum(y_dist, x_dist)\n","            region_weights = np.clip(1.0 - dist, 0.1, 1.0)\n","\n","            # Convert region_best_z to float64 before adding\n","            region_best_z_float = region_best_z.astype(np.float64)\n","\n","            # Add weighted contribution to output\n","            best_z[y_start:y_end, x_start:x_end] += region_best_z_float * region_weights\n","            weights[y_start:y_end, x_start:x_end] += region_weights\n","\n","    # Normalize by weights and round to nearest integer\n","    best_z = np.round(best_z / np.maximum(weights, 1e-6)).astype(np.int32)\n","\n","    # Ensure best_z values are within valid range\n","    best_z = np.clip(best_z, 0, z_size - 1)\n","\n","    # Refine the z-map\n","    print(\"Refining z-map...\")\n","    refined_best_z = refine_z_map(best_z, focus_measures, max_refinement_dist)\n","\n","    # Create output by taking pixels from best z-slices\n","    print(\"Creating final output...\")\n","    result = np.zeros((height, width), dtype=np.float32)\n","    for z in range(z_size):\n","        mask = refined_best_z == z\n","        result[mask] = stack[z][mask]\n","\n","    # Create a visualization of the focus map (normalized to 0-1)\n","    focus_map = refined_best_z / (z_size - 1)\n","\n","    return result, focus_map, refined_best_z\n","\n","# ======= MAIN PROCESSING FUNCTIONS =======\n","\n","def process_single_stack(stack, apply_tophat=False, max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                        max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST):\n","    \"\"\"\n","    Process a single z-stack with the improved regional EDF method\n","    \"\"\"\n","    print(f\"Processing stack with shape {stack.shape}...\")\n","\n","    # Convert to float and ensure valid range (0-1)\n","    stack = img_as_float(stack)\n","\n","    # Ensure stack values are in 0-1 range\n","    min_val = np.min(stack)\n","    max_val = np.max(stack)\n","    if min_val < 0 or max_val > 1:\n","        print(f\"Normalizing stack from range [{min_val}, {max_val}] to [0, 1]\")\n","        stack = normalize_image(stack)\n","\n","    # Apply background removal if requested\n","    if apply_tophat:\n","        print(\"Applying tophat background removal...\")\n","        processed_stack = np.zeros_like(stack, dtype=np.float32)\n","        for z in range(stack.shape[0]):\n","            processed_stack[z] = tophat_filter_background(stack[z])\n","        working_stack = processed_stack\n","    else:\n","        working_stack = stack\n","\n","    # Store original slices for comparison\n","    mid_z_idx = stack.shape[0] // 2\n","\n","    # Prepare results dictionary\n","    results = {\n","        'original_middle': stack[mid_z_idx],\n","        'original_max': np.max(stack, axis=0)\n","    }\n","\n","    # Apply improved regional method\n","    print(\"Applying improved regional guided EDF...\")\n","    proj, focus_map, _ = improved_edf_regional_guided(\n","        working_stack, max_z_diff=max_z_diff, max_refinement_dist=max_refinement_dist)\n","    results['regional_edf'] = proj\n","    results['focus_map_regional'] = focus_map\n","\n","    return results\n","\n","def save_results_to_disk(results, filename, channel_name, apply_tophat):\n","    \"\"\"Save results to disk with updated directory structure and naming\"\"\"\n","    base_name = os.path.splitext(filename)[0]\n","\n","    # Get only the projection result (not focus map)\n","    key = \"regional_edf\"\n","    if key in results:\n","        # Determine processing type folder\n","        process_type = \"tophat\" if apply_tophat else \"background\"\n","\n","        # Skip if this is not one of the required outputs\n","        if channel_name == \"Nuclei\" and not apply_tophat:\n","            print(f\"Skipping {channel_name} without tophat (not required)\")\n","            return\n","\n","        # Create appropriate directory path\n","        output_dir = os.path.join(OUTPUT_DIR, channel_name, process_type)\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Create output filename - using just the base name as requested\n","        output_filename = f\"{base_name}_{channel_name}_regional\"\n","        if apply_tophat:\n","            output_filename += \"_tophat\"\n","        output_filename += \".tif\"\n","\n","        output_path = os.path.join(output_dir, output_filename)\n","\n","        # Normalize to 0-1 range and convert to uint16\n","        projection = results[key]\n","        projection_normalized = normalize_image(projection)\n","        projection_uint = img_as_uint(projection_normalized)\n","\n","        # Save\n","        tifffile.imwrite(output_path, projection_uint)\n","        print(f\"Saved {output_path}\")\n","\n","        # Optionally save focus map for analysis\n","        if 'focus_map_regional' in results:\n","            focus_map = results['focus_map_regional']\n","            focus_map_normalized = normalize_image(focus_map)\n","            focus_map_uint = img_as_uint(focus_map_normalized)\n","\n","            focus_map_filename = f\"{base_name}_{channel_name}_focus_map\"\n","            if apply_tophat:\n","                focus_map_filename += \"_tophat\"\n","            focus_map_filename += \".tif\"\n","\n","            focus_map_path = os.path.join(output_dir, focus_map_filename)\n","            tifffile.imwrite(focus_map_path, focus_map_uint)\n","            print(f\"Saved focus map: {focus_map_path}\")\n","\n","def process_directory(input_dir=INPUT_DIR, max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                     max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST,\n","                     file_pattern='*.tif*', save_results=True):\n","    \"\"\"\n","    Process all files in a directory with channel-specific settings\n","    \"\"\"\n","    # Get all matching files\n","    all_files = glob.glob(os.path.join(input_dir, file_pattern), recursive=True)\n","    all_files.sort()\n","\n","    if not all_files:\n","        print(f\"No files matching '{file_pattern}' found in {input_dir}\")\n","        return []\n","\n","    print(f\"Found {len(all_files)} files to process\")\n","\n","    # Create output directories\n","    create_dirs()\n","\n","    # Process each file\n","    processed_files = []\n","\n","    for file_idx, file_path in enumerate(all_files):\n","        print(f\"\\nProcessing file {file_idx+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n","\n","        try:\n","            results = process_file(\n","                file_path,\n","                max_z_diff=max_z_diff,\n","                max_refinement_dist=max_refinement_dist,\n","                save_results=save_results\n","            )\n","\n","            if results:\n","                processed_files.append(file_path)\n","                print(f\"Successfully processed {os.path.basename(file_path)}\")\n","\n","        except Exception as e:\n","            print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    print(f\"\\nSuccessfully processed {len(processed_files)}/{len(all_files)} files\")\n","    return processed_files\n","\n","def process_file(file_path, max_z_diff=DEFAULT_MAX_Z_DIFF,\n","                max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST,\n","                save_results=True):\n","    \"\"\"\n","    Process a single file with channel-specific processing:\n","    - Nuclei (channel 1): Apply tophat only\n","    - Cadherins and Golgi (channels 0 and 2): Create with and without tophat\n","    \"\"\"\n","    filename = os.path.basename(file_path)\n","    print(f\"Processing file: {filename}\")\n","\n","    # Load image\n","    try:\n","        image = tifffile.imread(file_path)\n","        print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n","    except Exception as e:\n","        print(f\"Error loading image: {e}\")\n","        return None\n","\n","    # Handle different dimensionalities\n","    if len(image.shape) == 3:\n","        # Single channel z-stack - process with and without tophat\n","        results_no_tophat = process_single_stack(\n","            image, apply_tophat=False,\n","            max_z_diff=max_z_diff,\n","            max_refinement_dist=max_refinement_dist)\n","\n","        results_tophat = process_single_stack(\n","            image, apply_tophat=True,\n","            max_z_diff=max_z_diff,\n","            max_refinement_dist=max_refinement_dist)\n","\n","        if save_results:\n","            save_results_to_disk(results_no_tophat, filename, \"default\", False)\n","            save_results_to_disk(results_tophat, filename, \"default\", True)\n","\n","        return {\"default_no_tophat\": results_no_tophat, \"default_tophat\": results_tophat}\n","\n","    elif len(image.shape) == 4:\n","        # Multi-channel z-stack with channel-specific processing\n","        all_results = {}\n","\n","        # Process each channel with appropriate settings\n","        for ch_idx in range(image.shape[0]):\n","            # Extract channel\n","            channel_data = image[ch_idx]\n","            ch_name = CHANNEL_NAMES[ch_idx] if ch_idx < len(CHANNEL_NAMES) else f\"channel_{ch_idx}\"\n","\n","            # Apply channel-specific processing\n","            if ch_idx == 1:  # Nuclei (second channel) - tophat only\n","                print(f\"Processing {ch_name} with tophat...\")\n","                results = process_single_stack(\n","                    channel_data, apply_tophat=True,\n","                    max_z_diff=max_z_diff,\n","                    max_refinement_dist=max_refinement_dist)\n","\n","                all_results[f\"{ch_name}_tophat\"] = results\n","\n","                if save_results:\n","                    save_results_to_disk(results, filename, ch_name, True)\n","\n","            else:  # Cadherins and Golgi (channels 0 and 2) - with and without tophat\n","                print(f\"Processing {ch_name} without tophat...\")\n","                results_no_tophat = process_single_stack(\n","                    channel_data, apply_tophat=False,\n","                    max_z_diff=max_z_diff,\n","                    max_refinement_dist=max_refinement_dist)\n","\n","                print(f\"Processing {ch_name} with tophat...\")\n","                results_tophat = process_single_stack(\n","                    channel_data, apply_tophat=True,\n","                    max_z_diff=max_z_diff,\n","                    max_refinement_dist=max_refinement_dist)\n","\n","                all_results[f\"{ch_name}_no_tophat\"] = results_no_tophat\n","                all_results[f\"{ch_name}_tophat\"] = results_tophat\n","\n","                if save_results:\n","                    save_results_to_disk(results_no_tophat, filename, ch_name, False)\n","                    save_results_to_disk(results_tophat, filename, ch_name, True)\n","\n","        return all_results\n","\n","    else:\n","        print(f\"Unsupported image dimensions: {len(image.shape)}\")\n","        return None\n","\n","# ======= VISUALIZATION FUNCTIONS =======\n","\n","def visualize_focus_comparison(original_stack, improved_result, original_best_z=None, improved_best_z=None):\n","    \"\"\"\n","    Visualize the comparison between original and improved methods\n","    \"\"\"\n","    z_size = original_stack.shape[0]\n","\n","    # Create figure\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","\n","    # Original middle slice\n","    mid_z = z_size // 2\n","    axes[0, 0].imshow(original_stack[mid_z], cmap='gray')\n","    axes[0, 0].set_title(f'Original Middle Slice (z={mid_z})')\n","\n","    # Original max projection\n","    axes[0, 1].imshow(np.max(original_stack, axis=0), cmap='gray')\n","    axes[0, 1].set_title('Original Max Projection')\n","\n","    # Original best z map if available\n","    if original_best_z is not None:\n","        original_focus_map = original_best_z / (z_size - 1)\n","        axes[0, 2].imshow(original_focus_map, cmap='viridis')\n","        axes[0, 2].set_title('Original Focus Map')\n","    else:\n","        axes[0, 2].set_visible(False)\n","\n","    # Improved result\n","    axes[1, 0].imshow(improved_result, cmap='gray')\n","    axes[1, 0].set_title('Improved EDF Result')\n","\n","    # Difference\n","    diff = normalize_image(np.abs(improved_result - np.max(original_stack, axis=0)))\n","    axes[1, 1].imshow(diff, cmap='hot')\n","    axes[1, 1].set_title('Difference (Improved vs Max Proj)')\n","\n","    # Improved best z map if available\n","    if improved_best_z is not None:\n","        improved_focus_map = improved_best_z / (z_size - 1)\n","        axes[1, 2].imshow(improved_focus_map, cmap='viridis')\n","        axes[1, 2].set_title('Improved Focus Map')\n","    else:\n","        axes[1, 2].set_visible(False)\n","\n","    # Remove ticks\n","    for ax in axes.ravel():\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","    plt.tight_layout()\n","    return fig\n","\n","# Main execution - just call process_directory with your parameters\n","def main():\n","    create_dirs()\n","    processed_files = process_directory(\n","        input_dir=INPUT_DIR,\n","        max_z_diff=DEFAULT_MAX_Z_DIFF,\n","        max_refinement_dist=DEFAULT_MAX_REFINEMENT_DIST,\n","        file_pattern='*.tif*',\n","        save_results=True\n","    )\n","    print(f\"Processed {len(processed_files)} files\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb4AlGT5IsMj","executionInfo":{"status":"ok","timestamp":1744120914769,"user_tz":-120,"elapsed":1148920,"user":{"displayName":"Guido Putignano","userId":"13974663347531370398"}},"outputId":"f5e49721-3398-452f-eeec-66d3393b12be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Found 1 files to process\n","\n","Processing file 1/1: denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004.tif\n","Processing file: denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004.tif\n","Image shape: (3, 13, 1024, 1024), dtype: float32\n","Processing Cadherins without tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.012688316404819489, 1.9504793882369995] to [0, 1]\n","Applying improved regional guided EDF...\n","Calculating multi-scale focus measures...\n","Determining adaptive region sizes...\n","Processing in adaptive regions...\n","Refining z-map...\n","Creating final output...\n","Processing Cadherins with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.012688316404819489, 1.9504793882369995] to [0, 1]\n","Applying tophat background removal...\n","Applying improved regional guided EDF...\n","Calculating multi-scale focus measures...\n","Determining adaptive region sizes...\n","Processing in adaptive regions...\n","Refining z-map...\n","Creating final output...\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Cadherins/background/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Cadherins_regional.tif\n","Saved focus map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Cadherins/background/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Cadherins_focus_map.tif\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Cadherins/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Cadherins_regional_tophat.tif\n","Saved focus map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Cadherins/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Cadherins_focus_map_tophat.tif\n","Processing Nuclei with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.011354738846421242, 3.3957269191741943] to [0, 1]\n","Applying tophat background removal...\n","Applying improved regional guided EDF...\n","Calculating multi-scale focus measures...\n","Determining adaptive region sizes...\n","Processing in adaptive regions...\n","Refining z-map...\n","Creating final output...\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Nuclei/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Nuclei_regional_tophat.tif\n","Saved focus map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Nuclei/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Nuclei_focus_map_tophat.tif\n","Processing Golgi without tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.008244904689490795, 4.856940746307373] to [0, 1]\n","Applying improved regional guided EDF...\n","Calculating multi-scale focus measures...\n","Determining adaptive region sizes...\n","Processing in adaptive regions...\n","Refining z-map...\n","Creating final output...\n","Processing Golgi with tophat...\n","Processing stack with shape (13, 1024, 1024)...\n","Normalizing stack from range [-0.008244904689490795, 4.856940746307373] to [0, 1]\n","Applying tophat background removal...\n","Applying improved regional guided EDF...\n","Calculating multi-scale focus measures...\n","Determining adaptive region sizes...\n","Processing in adaptive regions...\n","Refining z-map...\n","Creating final output...\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Golgi/background/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Golgi_regional.tif\n","Saved focus map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Golgi/background/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Golgi_focus_map.tif\n","Saved /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Golgi/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Golgi_regional_tophat.tif\n","Saved focus map: /content/drive/MyDrive/knowledge/University/Master/Thesis/Projected/trial_tmp4/Golgi/tophat/denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004_Golgi_focus_map_tophat.tif\n","Successfully processed denoised_1.4Pa_A1_20dec21_20xA_L2RA_FlatA_seq004.tif\n","\n","Successfully processed 1/1 files\n","Processed 1 files\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"11s3RnUFngj9Z54WPSOXkw63-pPXarg6-","authorship_tag":"ABX9TyMek+NX1pwZh1Hn5TU2VJnp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}